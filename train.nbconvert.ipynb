{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-11T10:19:42.256653Z",
     "iopub.status.busy": "2023-01-11T10:19:42.255990Z",
     "iopub.status.idle": "2023-01-11T10:19:43.595087Z",
     "shell.execute_reply": "2023-01-11T10:19:43.594611Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wahmed/.local/lib/python3.8/site-packages/tensorflow_addons/utils/ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.9.0 and strictly below 2.12.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.8.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import ast\n",
    "import logging\n",
    "import string\n",
    "import random\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "\n",
    "from dimenet.model.dimenet import DimeNet\n",
    "from dimenet.model.dimenet_pp import DimeNetPP\n",
    "from dimenet.model.activations import swish\n",
    "from dimenet.training.trainer import Trainer\n",
    "from dimenet.training.metrics import Metrics\n",
    "from dimenet.training.data_container import DataContainer\n",
    "from dimenet.training.data_provider import DataProvider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-11T10:19:43.617826Z",
     "iopub.status.busy": "2023-01-11T10:19:43.617452Z",
     "iopub.status.idle": "2023-01-11T10:19:43.620924Z",
     "shell.execute_reply": "2023-01-11T10:19:43.620471Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set up logger\n",
    "logger = logging.getLogger()\n",
    "logger.handlers = []\n",
    "ch = logging.StreamHandler()\n",
    "formatter = logging.Formatter(\n",
    "        fmt='%(asctime)s (%(levelname)s): %(message)s',\n",
    "        datefmt='%Y-%m-%d %H:%M:%S')\n",
    "ch.setFormatter(formatter)\n",
    "logger.addHandler(ch)\n",
    "logger.setLevel('INFO')\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'\n",
    "tf.get_logger().setLevel('WARN')\n",
    "tf.autograph.set_verbosity(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-11T10:19:43.622588Z",
     "iopub.status.busy": "2023-01-11T10:19:43.622456Z",
     "iopub.status.idle": "2023-01-11T10:19:43.627082Z",
     "shell.execute_reply": "2023-01-11T10:19:43.626633Z"
    }
   },
   "outputs": [],
   "source": [
    "# config.yaml for DimeNet, config_pp.yaml for DimeNet++\n",
    "with open('config_pp.yaml', 'r') as c:\n",
    "    config = yaml.safe_load(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-11T10:19:43.628726Z",
     "iopub.status.busy": "2023-01-11T10:19:43.628593Z",
     "iopub.status.idle": "2023-01-11T10:19:43.631229Z",
     "shell.execute_reply": "2023-01-11T10:19:43.630753Z"
    }
   },
   "outputs": [],
   "source": [
    "# For strings that yaml doesn't parse (e.g. None)\n",
    "for key, val in config.items():\n",
    "    if type(val) is str:\n",
    "        try:\n",
    "            config[key] = ast.literal_eval(val)\n",
    "        except (ValueError, SyntaxError):\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-11T10:19:43.632862Z",
     "iopub.status.busy": "2023-01-11T10:19:43.632730Z",
     "iopub.status.idle": "2023-01-11T10:19:43.637867Z",
     "shell.execute_reply": "2023-01-11T10:19:43.637390Z"
    }
   },
   "outputs": [],
   "source": [
    "model_name = config['model_name']\n",
    "\n",
    "if model_name == \"dimenet\":\n",
    "    num_bilinear = config['num_bilinear']\n",
    "elif model_name == \"dimenet++\":\n",
    "    out_emb_size = config['out_emb_size']\n",
    "    int_emb_size = config['int_emb_size']\n",
    "    basis_emb_size = config['basis_emb_size']\n",
    "    extensive = config['extensive']\n",
    "else:\n",
    "    raise ValueError(f\"Unknown model name: '{model_name}'\")\n",
    "    \n",
    "emb_size = config['emb_size']\n",
    "num_blocks = config['num_blocks']\n",
    "\n",
    "num_spherical = config['num_spherical']\n",
    "num_radial = config['num_radial']\n",
    "output_init = config['output_init']\n",
    "\n",
    "cutoff = config['cutoff']\n",
    "envelope_exponent = config['envelope_exponent']\n",
    "\n",
    "num_before_skip = config['num_before_skip']\n",
    "num_after_skip = config['num_after_skip']\n",
    "num_dense_output = config['num_dense_output']\n",
    "\n",
    "num_train = config['num_train']\n",
    "num_valid = config['num_valid']\n",
    "data_seed = config['data_seed']\n",
    "dataset = config['dataset']\n",
    "logdir = config['logdir']\n",
    "\n",
    "num_steps = config['num_steps']\n",
    "ema_decay = config['ema_decay']\n",
    "\n",
    "learning_rate = config['learning_rate']\n",
    "warmup_steps = config['warmup_steps']\n",
    "decay_rate = config['decay_rate']\n",
    "decay_steps = config['decay_steps']\n",
    "\n",
    "batch_size = config['batch_size']\n",
    "evaluation_interval = config['evaluation_interval']\n",
    "save_interval = config['save_interval']\n",
    "restart = config['restart']\n",
    "comment = config['comment']\n",
    "targets = config['targets']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-11T10:19:43.639540Z",
     "iopub.status.busy": "2023-01-11T10:19:43.639409Z",
     "iopub.status.idle": "2023-01-11T10:19:43.643984Z",
     "shell.execute_reply": "2023-01-11T10:19:43.643670Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-11 11:19:43 (INFO): Directory: ./20230111_111943_mKVadACh_qm9_eV.npz_U0_final\n"
     ]
    }
   ],
   "source": [
    "# Used for creating a random \"unique\" id for this run\n",
    "def id_generator(size=8, chars=string.ascii_uppercase + string.ascii_lowercase + string.digits):\n",
    "    return ''.join(random.SystemRandom().choice(chars) for _ in range(size))\n",
    "\n",
    "# Create directories\n",
    "# A unique directory name is created for this run based on the input\n",
    "if restart is None:\n",
    "    directory = (logdir + \"/\" + datetime.now().strftime(\"%Y%m%d_%H%M%S\") + \"_\" + id_generator()\n",
    "                 + \"_\" + os.path.basename(dataset)\n",
    "                 + \"_\" + '-'.join(targets)\n",
    "                 + \"_\" + comment)\n",
    "else:\n",
    "    directory = restart\n",
    "logging.info(f\"Directory: {directory}\")\n",
    "\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "best_dir = os.path.join(directory, 'best')\n",
    "if not os.path.exists(best_dir):\n",
    "    os.makedirs(best_dir)\n",
    "log_dir = os.path.join(directory, 'logs')\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "best_loss_file = os.path.join(best_dir, 'best_loss.npz')\n",
    "best_ckpt_file = os.path.join(best_dir, 'ckpt')\n",
    "step_ckpt_folder = log_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create summary writer and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-11T10:19:43.645635Z",
     "iopub.status.busy": "2023-01-11T10:19:43.645505Z",
     "iopub.status.idle": "2023-01-11T10:19:44.617771Z",
     "shell.execute_reply": "2023-01-11T10:19:44.617257Z"
    }
   },
   "outputs": [],
   "source": [
    "summary_writer = tf.summary.create_file_writer(log_dir)\n",
    "\n",
    "train = {}\n",
    "validation = {}\n",
    "\n",
    "train['metrics'] = Metrics('train', targets)\n",
    "validation['metrics'] = Metrics('val', targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-11T10:19:44.619713Z",
     "iopub.status.busy": "2023-01-11T10:19:44.619575Z",
     "iopub.status.idle": "2023-01-11T10:19:44.772136Z",
     "shell.execute_reply": "2023-01-11T10:19:44.771620Z"
    }
   },
   "outputs": [],
   "source": [
    "data_container = DataContainer(dataset, cutoff=cutoff, target_keys=targets)\n",
    "\n",
    "# Initialize DataProvider (splits dataset into 3 sets based on data_seed and provides tf.datasets)\n",
    "data_provider = DataProvider(data_container, num_train, num_valid, batch_size,\n",
    "                             seed=data_seed, randomized=True)\n",
    "\n",
    "# Initialize datasets\n",
    "train['dataset'] = data_provider.get_dataset('train').prefetch(tf.data.experimental.AUTOTUNE)\n",
    "train['dataset_iter'] = iter(train['dataset'])\n",
    "validation['dataset'] = data_provider.get_dataset('val').prefetch(tf.data.experimental.AUTOTUNE)\n",
    "validation['dataset_iter'] = iter(validation['dataset'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-11T10:19:44.774070Z",
     "iopub.status.busy": "2023-01-11T10:19:44.773930Z",
     "iopub.status.idle": "2023-01-11T10:19:56.779333Z",
     "shell.execute_reply": "2023-01-11T10:19:56.778808Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dimenet++\n"
     ]
    }
   ],
   "source": [
    "if model_name == \"dimenet\":\n",
    "    print(\"dimenet\")\n",
    "    model = DimeNet(\n",
    "            emb_size=emb_size, num_blocks=num_blocks, num_bilinear=num_bilinear,\n",
    "            num_spherical=num_spherical, num_radial=num_radial,\n",
    "            cutoff=cutoff, envelope_exponent=envelope_exponent,\n",
    "            num_before_skip=num_before_skip, num_after_skip=num_after_skip,\n",
    "            num_dense_output=num_dense_output, num_targets=len(targets),\n",
    "            activation=swish, output_init=output_init)\n",
    "elif model_name == \"dimenet++\":\n",
    "    print(\"dimenet++\")\n",
    "    model = DimeNetPP(\n",
    "            emb_size=emb_size, out_emb_size=out_emb_size,\n",
    "            int_emb_size=int_emb_size, basis_emb_size=basis_emb_size,\n",
    "            num_blocks=num_blocks, num_spherical=num_spherical, num_radial=num_radial,\n",
    "            cutoff=cutoff, envelope_exponent=envelope_exponent,\n",
    "            num_before_skip=num_before_skip, num_after_skip=num_after_skip,\n",
    "            num_dense_output=num_dense_output, num_targets=len(targets),\n",
    "            activation=swish, extensive=extensive, output_init=output_init)\n",
    "else:\n",
    "    raise ValueError(f\"Unknown model name: '{model_name}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save/load best recorded loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-11T10:19:56.781252Z",
     "iopub.status.busy": "2023-01-11T10:19:56.781107Z",
     "iopub.status.idle": "2023-01-11T10:19:56.786552Z",
     "shell.execute_reply": "2023-01-11T10:19:56.786101Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wahmed/codes/dompe/dimenet/dimenet/training/metrics.py:68: RuntimeWarning: divide by zero encountered in log\n",
      "  return np.mean(np.log(self.maes)).item()\n"
     ]
    }
   ],
   "source": [
    "if os.path.isfile(best_loss_file):\n",
    "    loss_file = np.load(best_loss_file)\n",
    "    metrics_best = {k: v.item() for k, v in loss_file.items()}\n",
    "else:\n",
    "    metrics_best = validation['metrics'].result()\n",
    "    for key in metrics_best.keys():\n",
    "        metrics_best[key] = np.inf\n",
    "    metrics_best['step'] = 0\n",
    "    np.savez(best_loss_file, **metrics_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-11T10:19:56.788259Z",
     "iopub.status.busy": "2023-01-11T10:19:56.788106Z",
     "iopub.status.idle": "2023-01-11T10:19:56.790829Z",
     "shell.execute_reply": "2023-01-11T10:19:56.790381Z"
    }
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(model, learning_rate, warmup_steps,\n",
    "                  decay_steps, decay_rate,\n",
    "                  ema_decay=ema_decay, max_grad_norm=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up checkpointing and load latest checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-11T10:19:56.792471Z",
     "iopub.status.busy": "2023-01-11T10:19:56.792340Z",
     "iopub.status.idle": "2023-01-11T10:19:56.795647Z",
     "shell.execute_reply": "2023-01-11T10:19:56.795223Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set up checkpointing\n",
    "ckpt = tf.train.Checkpoint(step=tf.Variable(1), optimizer=trainer.optimizer, model=model)\n",
    "manager = tf.train.CheckpointManager(ckpt, step_ckpt_folder, max_to_keep=3)\n",
    "\n",
    "# Restore latest checkpoint\n",
    "ckpt_restored = tf.train.latest_checkpoint(log_dir)\n",
    "if ckpt_restored is not None:\n",
    "    ckpt.restore(ckpt_restored)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop\n",
    "\n",
    "Note that the warning `UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.` is expected. It is due to the backward pass of `tf.gather` ([this one](https://github.com/klicperajo/dimenet/blob/master/dimenet/model/layers/interaction_block.py#L62), to be exact) producing sparse gradients, which the previous layer has to convert to a dense tensor (see [this Stack Overflow question](https://stackoverflow.com/questions/35892412/tensorflow-dense-gradient-explanation))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-11T10:19:56.797306Z",
     "iopub.status.busy": "2023-01-11T10:19:56.797176Z",
     "iopub.status.idle": "2023-01-12T15:23:25.328633Z",
     "shell.execute_reply": "2023-01-12T15:23:25.328106Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wahmed/.local/lib/python3.8/site-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/dimenet/interaction/Reshape_2:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/dimenet/interaction/Reshape_1:0\", shape=(None, 64), dtype=float32), dense_shape=Tensor(\"gradient_tape/dimenet/interaction/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/home/wahmed/.local/lib/python3.8/site-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/dimenet/interaction/Reshape_5:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/dimenet/interaction/Reshape_4:0\", shape=(None, 64), dtype=float32), dense_shape=Tensor(\"gradient_tape/dimenet/interaction/Cast_1:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/home/wahmed/.local/lib/python3.8/site-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/dimenet/interaction/Reshape_8:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/dimenet/interaction/Reshape_7:0\", shape=(None, 64), dtype=float32), dense_shape=Tensor(\"gradient_tape/dimenet/interaction/Cast_2:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/home/wahmed/.local/lib/python3.8/site-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/dimenet/interaction/Reshape_11:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/dimenet/interaction/Reshape_10:0\", shape=(None, 64), dtype=float32), dense_shape=Tensor(\"gradient_tape/dimenet/interaction/Cast_3:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "2023-01-11 11:25:53 (INFO): 10000/3000000 (epoch 3): Loss: train=0.707247, val=0.083512; logMAE: train=-0.346375, val=-2.482762\n",
      "2023-01-11 11:31:43 (INFO): 20000/3000000 (epoch 6): Loss: train=0.211727, val=0.058283; logMAE: train=-1.552457, val=-2.842451\n",
      "2023-01-11 11:37:34 (INFO): 30000/3000000 (epoch 9): Loss: train=0.162896, val=0.047214; logMAE: train=-1.814643, val=-3.053075\n",
      "2023-01-11 11:43:27 (INFO): 40000/3000000 (epoch 12): Loss: train=0.139717, val=0.041799; logMAE: train=-1.968137, val=-3.174887\n",
      "2023-01-11 11:49:19 (INFO): 50000/3000000 (epoch 15): Loss: train=0.121542, val=0.037428; logMAE: train=-2.107493, val=-3.285325\n",
      "2023-01-11 11:55:08 (INFO): 60000/3000000 (epoch 18): Loss: train=0.106257, val=0.034055; logMAE: train=-2.241897, val=-3.379790\n",
      "2023-01-11 12:00:57 (INFO): 70000/3000000 (epoch 21): Loss: train=0.098130, val=0.031495; logMAE: train=-2.321460, val=-3.457913\n",
      "2023-01-11 12:06:46 (INFO): 80000/3000000 (epoch 24): Loss: train=0.089060, val=0.029251; logMAE: train=-2.418441, val=-3.531849\n",
      "2023-01-11 12:12:37 (INFO): 90000/3000000 (epoch 27): Loss: train=0.084888, val=0.027946; logMAE: train=-2.466417, val=-3.577464\n",
      "2023-01-11 12:18:29 (INFO): 100000/3000000 (epoch 30): Loss: train=0.079859, val=0.026271; logMAE: train=-2.527495, val=-3.639301\n",
      "2023-01-11 12:24:19 (INFO): 110000/3000000 (epoch 32): Loss: train=0.074813, val=0.024922; logMAE: train=-2.592770, val=-3.692014\n",
      "2023-01-11 12:30:09 (INFO): 120000/3000000 (epoch 35): Loss: train=0.068815, val=0.024398; logMAE: train=-2.676333, val=-3.713255\n",
      "2023-01-11 12:36:00 (INFO): 130000/3000000 (epoch 38): Loss: train=0.067246, val=0.023162; logMAE: train=-2.699403, val=-3.765221\n",
      "2023-01-11 12:41:51 (INFO): 140000/3000000 (epoch 41): Loss: train=0.065188, val=0.022551; logMAE: train=-2.730483, val=-3.791955\n",
      "2023-01-11 12:47:41 (INFO): 150000/3000000 (epoch 44): Loss: train=0.062113, val=0.022685; logMAE: train=-2.778800, val=-3.786058\n",
      "2023-01-11 12:53:32 (INFO): 160000/3000000 (epoch 47): Loss: train=0.059162, val=0.021318; logMAE: train=-2.827469, val=-3.848204\n",
      "2023-01-11 12:59:23 (INFO): 170000/3000000 (epoch 50): Loss: train=0.056404, val=0.020482; logMAE: train=-2.875223, val=-3.888204\n",
      "2023-01-11 13:05:14 (INFO): 180000/3000000 (epoch 53): Loss: train=0.055000, val=0.020133; logMAE: train=-2.900418, val=-3.905375\n",
      "2023-01-11 13:11:06 (INFO): 190000/3000000 (epoch 56): Loss: train=0.052704, val=0.019526; logMAE: train=-2.943058, val=-3.936023\n",
      "2023-01-11 13:16:57 (INFO): 200000/3000000 (epoch 59): Loss: train=0.051860, val=0.019331; logMAE: train=-2.959205, val=-3.946030\n",
      "2023-01-11 13:22:49 (INFO): 210000/3000000 (epoch 62): Loss: train=0.049445, val=0.018763; logMAE: train=-3.006896, val=-3.975850\n",
      "2023-01-11 13:28:41 (INFO): 220000/3000000 (epoch 64): Loss: train=0.047040, val=0.018110; logMAE: train=-3.056750, val=-4.011294\n",
      "2023-01-11 13:34:33 (INFO): 230000/3000000 (epoch 67): Loss: train=0.045548, val=0.017760; logMAE: train=-3.088990, val=-4.030811\n",
      "2023-01-11 13:40:25 (INFO): 240000/3000000 (epoch 70): Loss: train=0.044353, val=0.017341; logMAE: train=-3.115586, val=-4.054686\n",
      "2023-01-11 13:46:18 (INFO): 250000/3000000 (epoch 73): Loss: train=0.045363, val=0.017519; logMAE: train=-3.093061, val=-4.044441\n",
      "2023-01-11 13:52:10 (INFO): 260000/3000000 (epoch 76): Loss: train=0.042001, val=0.016887; logMAE: train=-3.170070, val=-4.081203\n",
      "2023-01-11 13:58:02 (INFO): 270000/3000000 (epoch 79): Loss: train=0.040943, val=0.016699; logMAE: train=-3.195571, val=-4.092422\n",
      "2023-01-11 14:03:54 (INFO): 280000/3000000 (epoch 82): Loss: train=0.040417, val=0.016809; logMAE: train=-3.208505, val=-4.085861\n",
      "2023-01-11 14:09:45 (INFO): 290000/3000000 (epoch 85): Loss: train=0.037807, val=0.016150; logMAE: train=-3.275257, val=-4.125832\n",
      "2023-01-11 14:15:37 (INFO): 300000/3000000 (epoch 88): Loss: train=0.039611, val=0.015774; logMAE: train=-3.228657, val=-4.149388\n",
      "2023-01-11 14:21:31 (INFO): 310000/3000000 (epoch 91): Loss: train=0.036704, val=0.015530; logMAE: train=-3.304868, val=-4.164957\n",
      "2023-01-11 14:27:23 (INFO): 320000/3000000 (epoch 94): Loss: train=0.036813, val=0.015403; logMAE: train=-3.301914, val=-4.173198\n",
      "2023-01-11 14:33:16 (INFO): 330000/3000000 (epoch 96): Loss: train=0.035253, val=0.015158; logMAE: train=-3.345216, val=-4.189201\n",
      "2023-01-11 14:39:09 (INFO): 340000/3000000 (epoch 99): Loss: train=0.034726, val=0.014720; logMAE: train=-3.360281, val=-4.218517\n",
      "2023-01-11 14:45:02 (INFO): 350000/3000000 (epoch 102): Loss: train=0.034055, val=0.014887; logMAE: train=-3.379777, val=-4.207273\n",
      "2023-01-11 14:50:56 (INFO): 360000/3000000 (epoch 105): Loss: train=0.033100, val=0.014500; logMAE: train=-3.408218, val=-4.233612\n",
      "2023-01-11 14:56:51 (INFO): 370000/3000000 (epoch 108): Loss: train=0.031946, val=0.014424; logMAE: train=-3.443697, val=-4.238879\n",
      "2023-01-11 15:02:45 (INFO): 380000/3000000 (epoch 111): Loss: train=0.032120, val=0.014144; logMAE: train=-3.438273, val=-4.258485\n",
      "2023-01-11 15:08:39 (INFO): 390000/3000000 (epoch 114): Loss: train=0.030982, val=0.014089; logMAE: train=-3.474340, val=-4.262369\n",
      "2023-01-11 15:14:30 (INFO): 400000/3000000 (epoch 117): Loss: train=0.030545, val=0.013880; logMAE: train=-3.488565, val=-4.277293\n",
      "2023-01-11 15:20:21 (INFO): 410000/3000000 (epoch 120): Loss: train=0.029251, val=0.013488; logMAE: train=-3.531849, val=-4.305943\n",
      "2023-01-11 15:26:10 (INFO): 420000/3000000 (epoch 123): Loss: train=0.029262, val=0.013462; logMAE: train=-3.531451, val=-4.307868\n",
      "2023-01-11 15:31:58 (INFO): 430000/3000000 (epoch 126): Loss: train=0.028215, val=0.013288; logMAE: train=-3.567898, val=-4.320900\n",
      "2023-01-11 15:37:47 (INFO): 440000/3000000 (epoch 128): Loss: train=0.028290, val=0.013171; logMAE: train=-3.565243, val=-4.329718\n",
      "2023-01-11 15:43:35 (INFO): 450000/3000000 (epoch 131): Loss: train=0.026717, val=0.013026; logMAE: train=-3.622456, val=-4.340810\n",
      "2023-01-11 15:49:23 (INFO): 460000/3000000 (epoch 134): Loss: train=0.027574, val=0.012941; logMAE: train=-3.590894, val=-4.347366\n",
      "2023-01-11 15:55:10 (INFO): 470000/3000000 (epoch 137): Loss: train=0.026086, val=0.012794; logMAE: train=-3.646356, val=-4.358789\n",
      "2023-01-11 16:00:58 (INFO): 480000/3000000 (epoch 140): Loss: train=0.025975, val=0.012718; logMAE: train=-3.650632, val=-4.364707\n",
      "2023-01-11 16:06:45 (INFO): 490000/3000000 (epoch 143): Loss: train=0.025480, val=0.012618; logMAE: train=-3.669871, val=-4.372615\n",
      "2023-01-11 16:12:33 (INFO): 500000/3000000 (epoch 146): Loss: train=0.025266, val=0.012436; logMAE: train=-3.678286, val=-4.387168\n",
      "2023-01-11 16:18:20 (INFO): 510000/3000000 (epoch 149): Loss: train=0.024531, val=0.012295; logMAE: train=-3.707803, val=-4.398582\n",
      "2023-01-11 16:24:08 (INFO): 520000/3000000 (epoch 152): Loss: train=0.024582, val=0.012262; logMAE: train=-3.705760, val=-4.401248\n",
      "2023-01-11 16:29:56 (INFO): 530000/3000000 (epoch 155): Loss: train=0.024361, val=0.012076; logMAE: train=-3.714785, val=-4.416559\n",
      "2023-01-11 16:35:44 (INFO): 540000/3000000 (epoch 158): Loss: train=0.023535, val=0.012005; logMAE: train=-3.749277, val=-4.422452\n",
      "2023-01-11 16:41:31 (INFO): 550000/3000000 (epoch 160): Loss: train=0.023245, val=0.012005; logMAE: train=-3.761668, val=-4.422462\n",
      "2023-01-11 16:47:19 (INFO): 560000/3000000 (epoch 163): Loss: train=0.023053, val=0.011767; logMAE: train=-3.769981, val=-4.442450\n",
      "2023-01-11 16:53:07 (INFO): 570000/3000000 (epoch 166): Loss: train=0.022600, val=0.011719; logMAE: train=-3.789785, val=-4.446539\n",
      "2023-01-11 16:58:55 (INFO): 580000/3000000 (epoch 169): Loss: train=0.022788, val=0.011692; logMAE: train=-3.781504, val=-4.448853\n",
      "2023-01-11 17:04:43 (INFO): 590000/3000000 (epoch 172): Loss: train=0.021575, val=0.011581; logMAE: train=-3.836209, val=-4.458403\n",
      "2023-01-11 17:10:30 (INFO): 600000/3000000 (epoch 175): Loss: train=0.021452, val=0.011380; logMAE: train=-3.841935, val=-4.475856\n",
      "2023-01-11 17:16:18 (INFO): 610000/3000000 (epoch 178): Loss: train=0.021418, val=0.011415; logMAE: train=-3.843508, val=-4.472850\n",
      "2023-01-11 17:22:06 (INFO): 620000/3000000 (epoch 181): Loss: train=0.021635, val=0.011215; logMAE: train=-3.833425, val=-4.490521\n",
      "2023-01-11 17:27:54 (INFO): 630000/3000000 (epoch 184): Loss: train=0.020607, val=0.011105; logMAE: train=-3.882108, val=-4.500343\n",
      "2023-01-11 17:33:42 (INFO): 640000/3000000 (epoch 187): Loss: train=0.020709, val=0.011105; logMAE: train=-3.877206, val=-4.500388\n",
      "2023-01-11 17:39:29 (INFO): 650000/3000000 (epoch 190): Loss: train=0.019864, val=0.010916; logMAE: train=-3.918834, val=-4.517565\n",
      "2023-01-11 17:45:17 (INFO): 660000/3000000 (epoch 192): Loss: train=0.019905, val=0.010972; logMAE: train=-3.916774, val=-4.512396\n",
      "2023-01-11 17:51:05 (INFO): 670000/3000000 (epoch 195): Loss: train=0.019277, val=0.010888; logMAE: train=-3.948868, val=-4.520080\n",
      "2023-01-11 17:56:53 (INFO): 680000/3000000 (epoch 198): Loss: train=0.019146, val=0.010863; logMAE: train=-3.955645, val=-4.522420\n",
      "2023-01-11 18:02:41 (INFO): 690000/3000000 (epoch 201): Loss: train=0.019417, val=0.010769; logMAE: train=-3.941587, val=-4.531075\n",
      "2023-01-11 18:08:29 (INFO): 700000/3000000 (epoch 204): Loss: train=0.019189, val=0.010687; logMAE: train=-3.953413, val=-4.538756\n",
      "2023-01-11 18:14:17 (INFO): 710000/3000000 (epoch 207): Loss: train=0.018443, val=0.010589; logMAE: train=-3.993063, val=-4.547977\n",
      "2023-01-11 18:20:05 (INFO): 720000/3000000 (epoch 210): Loss: train=0.018180, val=0.010456; logMAE: train=-4.007414, val=-4.560555\n",
      "2023-01-11 18:25:53 (INFO): 730000/3000000 (epoch 213): Loss: train=0.018212, val=0.010524; logMAE: train=-4.005695, val=-4.554101\n",
      "2023-01-11 18:31:42 (INFO): 740000/3000000 (epoch 216): Loss: train=0.017777, val=0.010453; logMAE: train=-4.029825, val=-4.560841\n",
      "2023-01-11 18:37:30 (INFO): 750000/3000000 (epoch 219): Loss: train=0.017298, val=0.010345; logMAE: train=-4.057181, val=-4.571211\n",
      "2023-01-11 18:43:19 (INFO): 760000/3000000 (epoch 222): Loss: train=0.017177, val=0.010378; logMAE: train=-4.064165, val=-4.568055\n",
      "2023-01-11 18:49:08 (INFO): 770000/3000000 (epoch 224): Loss: train=0.017008, val=0.010262; logMAE: train=-4.074074, val=-4.579304\n",
      "2023-01-11 18:54:56 (INFO): 780000/3000000 (epoch 227): Loss: train=0.016786, val=0.010114; logMAE: train=-4.087207, val=-4.593810\n",
      "2023-01-11 19:00:44 (INFO): 790000/3000000 (epoch 230): Loss: train=0.016732, val=0.010158; logMAE: train=-4.090427, val=-4.589528\n",
      "2023-01-11 19:06:32 (INFO): 800000/3000000 (epoch 233): Loss: train=0.016053, val=0.010043; logMAE: train=-4.131854, val=-4.600831\n",
      "2023-01-11 19:12:20 (INFO): 810000/3000000 (epoch 236): Loss: train=0.016231, val=0.010037; logMAE: train=-4.120812, val=-4.601472\n",
      "2023-01-11 19:18:08 (INFO): 820000/3000000 (epoch 239): Loss: train=0.016166, val=0.010017; logMAE: train=-4.124844, val=-4.603449\n",
      "2023-01-11 19:23:56 (INFO): 830000/3000000 (epoch 242): Loss: train=0.015409, val=0.009922; logMAE: train=-4.172801, val=-4.613008\n",
      "2023-01-11 19:29:44 (INFO): 840000/3000000 (epoch 245): Loss: train=0.015656, val=0.009938; logMAE: train=-4.156898, val=-4.611348\n",
      "2023-01-11 19:35:32 (INFO): 850000/3000000 (epoch 248): Loss: train=0.015483, val=0.009841; logMAE: train=-4.168017, val=-4.621212\n",
      "2023-01-11 19:41:20 (INFO): 860000/3000000 (epoch 251): Loss: train=0.015073, val=0.009908; logMAE: train=-4.194876, val=-4.614448\n",
      "2023-01-11 19:47:08 (INFO): 870000/3000000 (epoch 254): Loss: train=0.014978, val=0.009779; logMAE: train=-4.201153, val=-4.627541\n",
      "2023-01-11 19:52:56 (INFO): 880000/3000000 (epoch 256): Loss: train=0.014844, val=0.009699; logMAE: train=-4.210166, val=-4.635709\n",
      "2023-01-11 19:58:44 (INFO): 890000/3000000 (epoch 259): Loss: train=0.014548, val=0.009679; logMAE: train=-4.230324, val=-4.637829\n",
      "2023-01-11 20:04:32 (INFO): 900000/3000000 (epoch 262): Loss: train=0.014768, val=0.009630; logMAE: train=-4.215303, val=-4.642871\n",
      "2023-01-11 20:10:20 (INFO): 910000/3000000 (epoch 265): Loss: train=0.014148, val=0.009597; logMAE: train=-4.258169, val=-4.646274\n",
      "2023-01-11 20:16:08 (INFO): 920000/3000000 (epoch 268): Loss: train=0.014051, val=0.009579; logMAE: train=-4.265063, val=-4.648226\n",
      "2023-01-11 20:21:56 (INFO): 930000/3000000 (epoch 271): Loss: train=0.014096, val=0.009565; logMAE: train=-4.261835, val=-4.649651\n",
      "2023-01-11 20:27:43 (INFO): 940000/3000000 (epoch 274): Loss: train=0.013723, val=0.009467; logMAE: train=-4.288697, val=-4.659916\n",
      "2023-01-11 20:33:31 (INFO): 950000/3000000 (epoch 277): Loss: train=0.013656, val=0.009462; logMAE: train=-4.293557, val=-4.660479\n",
      "2023-01-11 20:39:18 (INFO): 960000/3000000 (epoch 280): Loss: train=0.013154, val=0.009466; logMAE: train=-4.331016, val=-4.660064\n",
      "2023-01-11 20:45:06 (INFO): 970000/3000000 (epoch 283): Loss: train=0.013347, val=0.009489; logMAE: train=-4.316455, val=-4.657634\n",
      "2023-01-11 20:50:55 (INFO): 980000/3000000 (epoch 286): Loss: train=0.012826, val=0.009277; logMAE: train=-4.356303, val=-4.680220\n",
      "2023-01-11 20:56:43 (INFO): 990000/3000000 (epoch 288): Loss: train=0.013151, val=0.009329; logMAE: train=-4.331292, val=-4.674628\n",
      "2023-01-11 21:02:31 (INFO): 1000000/3000000 (epoch 291): Loss: train=0.012992, val=0.009295; logMAE: train=-4.343455, val=-4.678228\n",
      "2023-01-11 21:08:19 (INFO): 1010000/3000000 (epoch 294): Loss: train=0.012417, val=0.009242; logMAE: train=-4.388698, val=-4.683988\n",
      "2023-01-11 21:14:07 (INFO): 1020000/3000000 (epoch 297): Loss: train=0.012249, val=0.009194; logMAE: train=-4.402289, val=-4.689216\n",
      "2023-01-11 21:19:55 (INFO): 1030000/3000000 (epoch 300): Loss: train=0.012026, val=0.009181; logMAE: train=-4.420688, val=-4.690581\n",
      "2023-01-11 21:25:43 (INFO): 1040000/3000000 (epoch 303): Loss: train=0.011795, val=0.009167; logMAE: train=-4.440065, val=-4.692127\n",
      "2023-01-11 21:31:31 (INFO): 1050000/3000000 (epoch 306): Loss: train=0.012078, val=0.009132; logMAE: train=-4.416349, val=-4.696016\n",
      "2023-01-11 21:37:19 (INFO): 1060000/3000000 (epoch 309): Loss: train=0.012009, val=0.009017; logMAE: train=-4.422099, val=-4.708604\n",
      "2023-01-11 21:43:07 (INFO): 1070000/3000000 (epoch 312): Loss: train=0.011818, val=0.009081; logMAE: train=-4.438090, val=-4.701536\n",
      "2023-01-11 21:48:55 (INFO): 1080000/3000000 (epoch 315): Loss: train=0.011325, val=0.009055; logMAE: train=-4.480747, val=-4.704439\n",
      "2023-01-11 21:54:43 (INFO): 1090000/3000000 (epoch 318): Loss: train=0.011562, val=0.009013; logMAE: train=-4.460033, val=-4.709102\n",
      "2023-01-11 22:00:32 (INFO): 1100000/3000000 (epoch 320): Loss: train=0.011052, val=0.008970; logMAE: train=-4.505185, val=-4.713870\n",
      "2023-01-11 22:06:20 (INFO): 1110000/3000000 (epoch 323): Loss: train=0.011120, val=0.008938; logMAE: train=-4.499034, val=-4.717459\n",
      "2023-01-11 22:12:08 (INFO): 1120000/3000000 (epoch 326): Loss: train=0.011111, val=0.008928; logMAE: train=-4.499778, val=-4.718508\n",
      "2023-01-11 22:17:56 (INFO): 1130000/3000000 (epoch 329): Loss: train=0.010893, val=0.008889; logMAE: train=-4.519640, val=-4.722983\n",
      "2023-01-11 22:23:44 (INFO): 1140000/3000000 (epoch 332): Loss: train=0.010616, val=0.008874; logMAE: train=-4.545392, val=-4.724668\n",
      "2023-01-11 22:29:32 (INFO): 1150000/3000000 (epoch 335): Loss: train=0.010756, val=0.008806; logMAE: train=-4.532265, val=-4.732361\n",
      "2023-01-11 22:35:21 (INFO): 1160000/3000000 (epoch 338): Loss: train=0.010591, val=0.008806; logMAE: train=-4.547733, val=-4.732266\n",
      "2023-01-11 22:41:09 (INFO): 1170000/3000000 (epoch 341): Loss: train=0.010620, val=0.008814; logMAE: train=-4.544994, val=-4.731467\n",
      "2023-01-11 22:46:57 (INFO): 1180000/3000000 (epoch 344): Loss: train=0.010546, val=0.008759; logMAE: train=-4.552002, val=-4.737728\n",
      "2023-01-11 22:52:45 (INFO): 1190000/3000000 (epoch 347): Loss: train=0.010021, val=0.008713; logMAE: train=-4.603054, val=-4.742996\n",
      "2023-01-11 22:58:33 (INFO): 1200000/3000000 (epoch 350): Loss: train=0.009897, val=0.008710; logMAE: train=-4.615573, val=-4.743236\n",
      "2023-01-11 23:04:21 (INFO): 1210000/3000000 (epoch 352): Loss: train=0.010027, val=0.008706; logMAE: train=-4.602456, val=-4.743731\n",
      "2023-01-11 23:10:10 (INFO): 1220000/3000000 (epoch 355): Loss: train=0.009865, val=0.008637; logMAE: train=-4.618811, val=-4.751729\n",
      "2023-01-11 23:15:58 (INFO): 1230000/3000000 (epoch 358): Loss: train=0.009525, val=0.008640; logMAE: train=-4.653853, val=-4.751302\n",
      "2023-01-11 23:21:46 (INFO): 1240000/3000000 (epoch 361): Loss: train=0.009701, val=0.008597; logMAE: train=-4.635536, val=-4.756392\n",
      "2023-01-11 23:27:34 (INFO): 1250000/3000000 (epoch 364): Loss: train=0.009647, val=0.008591; logMAE: train=-4.641152, val=-4.757025\n",
      "2023-01-11 23:33:22 (INFO): 1260000/3000000 (epoch 367): Loss: train=0.009358, val=0.008568; logMAE: train=-4.671520, val=-4.759746\n",
      "2023-01-11 23:39:11 (INFO): 1270000/3000000 (epoch 370): Loss: train=0.009267, val=0.008573; logMAE: train=-4.681246, val=-4.759126\n",
      "2023-01-11 23:44:59 (INFO): 1280000/3000000 (epoch 373): Loss: train=0.009206, val=0.008545; logMAE: train=-4.687865, val=-4.762385\n",
      "2023-01-11 23:50:47 (INFO): 1290000/3000000 (epoch 376): Loss: train=0.009157, val=0.008533; logMAE: train=-4.693253, val=-4.763811\n",
      "2023-01-11 23:56:35 (INFO): 1300000/3000000 (epoch 379): Loss: train=0.009023, val=0.008505; logMAE: train=-4.707952, val=-4.767061\n",
      "2023-01-12 00:02:23 (INFO): 1310000/3000000 (epoch 382): Loss: train=0.008904, val=0.008503; logMAE: train=-4.721244, val=-4.767373\n",
      "2023-01-12 00:08:12 (INFO): 1320000/3000000 (epoch 384): Loss: train=0.008847, val=0.008439; logMAE: train=-4.727694, val=-4.774938\n",
      "2023-01-12 00:14:00 (INFO): 1330000/3000000 (epoch 387): Loss: train=0.008578, val=0.008466; logMAE: train=-4.758584, val=-4.771646\n",
      "2023-01-12 00:19:48 (INFO): 1340000/3000000 (epoch 390): Loss: train=0.008853, val=0.008451; logMAE: train=-4.727048, val=-4.773433\n",
      "2023-01-12 00:25:36 (INFO): 1350000/3000000 (epoch 393): Loss: train=0.008677, val=0.008444; logMAE: train=-4.747069, val=-4.774258\n",
      "2023-01-12 00:31:24 (INFO): 1360000/3000000 (epoch 396): Loss: train=0.008322, val=0.008394; logMAE: train=-4.788903, val=-4.780281\n",
      "2023-01-12 00:37:12 (INFO): 1370000/3000000 (epoch 399): Loss: train=0.008184, val=0.008399; logMAE: train=-4.805522, val=-4.779586\n",
      "2023-01-12 00:43:00 (INFO): 1380000/3000000 (epoch 402): Loss: train=0.008363, val=0.008363; logMAE: train=-4.783965, val=-4.783893\n",
      "2023-01-12 00:48:48 (INFO): 1390000/3000000 (epoch 405): Loss: train=0.008360, val=0.008345; logMAE: train=-4.784331, val=-4.786076\n",
      "2023-01-12 00:54:36 (INFO): 1400000/3000000 (epoch 408): Loss: train=0.008157, val=0.008310; logMAE: train=-4.808822, val=-4.790250\n",
      "2023-01-12 01:00:24 (INFO): 1410000/3000000 (epoch 411): Loss: train=0.008057, val=0.008279; logMAE: train=-4.821258, val=-4.794002\n",
      "2023-01-12 01:06:12 (INFO): 1420000/3000000 (epoch 414): Loss: train=0.008024, val=0.008277; logMAE: train=-4.825261, val=-4.794329\n",
      "2023-01-12 01:12:00 (INFO): 1430000/3000000 (epoch 416): Loss: train=0.007876, val=0.008261; logMAE: train=-4.843983, val=-4.796164\n",
      "2023-01-12 01:17:48 (INFO): 1440000/3000000 (epoch 419): Loss: train=0.008052, val=0.008252; logMAE: train=-4.821794, val=-4.797341\n",
      "2023-01-12 01:23:37 (INFO): 1450000/3000000 (epoch 422): Loss: train=0.007769, val=0.008240; logMAE: train=-4.857596, val=-4.798773\n",
      "2023-01-12 01:29:24 (INFO): 1460000/3000000 (epoch 425): Loss: train=0.007712, val=0.008230; logMAE: train=-4.864924, val=-4.799955\n",
      "2023-01-12 01:35:12 (INFO): 1470000/3000000 (epoch 428): Loss: train=0.007565, val=0.008198; logMAE: train=-4.884157, val=-4.803842\n",
      "2023-01-12 01:41:01 (INFO): 1480000/3000000 (epoch 431): Loss: train=0.007589, val=0.008189; logMAE: train=-4.881097, val=-4.804963\n",
      "2023-01-12 01:46:48 (INFO): 1490000/3000000 (epoch 434): Loss: train=0.007400, val=0.008197; logMAE: train=-4.906224, val=-4.804011\n",
      "2023-01-12 01:52:37 (INFO): 1500000/3000000 (epoch 437): Loss: train=0.007349, val=0.008154; logMAE: train=-4.913209, val=-4.809296\n",
      "2023-01-12 01:58:25 (INFO): 1510000/3000000 (epoch 440): Loss: train=0.007380, val=0.008122; logMAE: train=-4.909008, val=-4.813179\n",
      "2023-01-12 02:04:13 (INFO): 1520000/3000000 (epoch 443): Loss: train=0.007319, val=0.008198; logMAE: train=-4.917217, val=-4.803874\n",
      "2023-01-12 02:10:01 (INFO): 1530000/3000000 (epoch 446): Loss: train=0.007099, val=0.008144; logMAE: train=-4.947744, val=-4.810420\n",
      "2023-01-12 02:15:49 (INFO): 1540000/3000000 (epoch 448): Loss: train=0.007119, val=0.008136; logMAE: train=-4.945039, val=-4.811511\n",
      "2023-01-12 02:21:37 (INFO): 1550000/3000000 (epoch 451): Loss: train=0.007133, val=0.008109; logMAE: train=-4.943039, val=-4.814735\n",
      "2023-01-12 02:27:25 (INFO): 1560000/3000000 (epoch 454): Loss: train=0.007090, val=0.008101; logMAE: train=-4.949040, val=-4.815718\n",
      "2023-01-12 02:33:13 (INFO): 1570000/3000000 (epoch 457): Loss: train=0.007042, val=0.008064; logMAE: train=-4.955838, val=-4.820370\n",
      "2023-01-12 02:39:01 (INFO): 1580000/3000000 (epoch 460): Loss: train=0.006857, val=0.008106; logMAE: train=-4.982449, val=-4.815124\n",
      "2023-01-12 02:44:49 (INFO): 1590000/3000000 (epoch 463): Loss: train=0.006857, val=0.008064; logMAE: train=-4.982492, val=-4.820341\n",
      "2023-01-12 02:50:37 (INFO): 1600000/3000000 (epoch 466): Loss: train=0.006836, val=0.008031; logMAE: train=-4.985530, val=-4.824387\n",
      "2023-01-12 02:56:26 (INFO): 1610000/3000000 (epoch 469): Loss: train=0.006665, val=0.008029; logMAE: train=-5.010913, val=-4.824645\n",
      "2023-01-12 03:02:14 (INFO): 1620000/3000000 (epoch 472): Loss: train=0.006465, val=0.008022; logMAE: train=-5.041278, val=-4.825572\n",
      "2023-01-12 03:08:03 (INFO): 1630000/3000000 (epoch 475): Loss: train=0.006541, val=0.007993; logMAE: train=-5.029709, val=-4.829174\n",
      "2023-01-12 03:13:51 (INFO): 1640000/3000000 (epoch 478): Loss: train=0.006697, val=0.007989; logMAE: train=-5.006061, val=-4.829658\n",
      "2023-01-12 03:19:39 (INFO): 1650000/3000000 (epoch 480): Loss: train=0.006552, val=0.007962; logMAE: train=-5.027912, val=-4.833041\n",
      "2023-01-12 03:25:27 (INFO): 1660000/3000000 (epoch 483): Loss: train=0.006395, val=0.007978; logMAE: train=-5.052299, val=-4.831050\n",
      "2023-01-12 03:31:15 (INFO): 1670000/3000000 (epoch 486): Loss: train=0.006462, val=0.007958; logMAE: train=-5.041884, val=-4.833593\n",
      "2023-01-12 03:37:03 (INFO): 1680000/3000000 (epoch 489): Loss: train=0.006368, val=0.007958; logMAE: train=-5.056433, val=-4.833608\n",
      "2023-01-12 03:42:52 (INFO): 1690000/3000000 (epoch 492): Loss: train=0.006371, val=0.007905; logMAE: train=-5.055963, val=-4.840318\n",
      "2023-01-12 03:48:40 (INFO): 1700000/3000000 (epoch 495): Loss: train=0.006134, val=0.007923; logMAE: train=-5.093852, val=-4.837954\n",
      "2023-01-12 03:54:28 (INFO): 1710000/3000000 (epoch 498): Loss: train=0.006103, val=0.007906; logMAE: train=-5.098958, val=-4.840119\n",
      "2023-01-12 04:00:16 (INFO): 1720000/3000000 (epoch 501): Loss: train=0.006258, val=0.007884; logMAE: train=-5.073927, val=-4.842865\n",
      "2023-01-12 04:06:05 (INFO): 1730000/3000000 (epoch 504): Loss: train=0.006060, val=0.007846; logMAE: train=-5.106084, val=-4.847768\n",
      "2023-01-12 04:11:53 (INFO): 1740000/3000000 (epoch 507): Loss: train=0.006071, val=0.007876; logMAE: train=-5.104155, val=-4.843886\n",
      "2023-01-12 04:17:41 (INFO): 1750000/3000000 (epoch 510): Loss: train=0.006071, val=0.007862; logMAE: train=-5.104260, val=-4.845705\n",
      "2023-01-12 04:23:29 (INFO): 1760000/3000000 (epoch 512): Loss: train=0.006056, val=0.007862; logMAE: train=-5.106679, val=-4.845673\n",
      "2023-01-12 04:29:17 (INFO): 1770000/3000000 (epoch 515): Loss: train=0.005986, val=0.007891; logMAE: train=-5.118352, val=-4.842031\n",
      "2023-01-12 04:35:05 (INFO): 1780000/3000000 (epoch 518): Loss: train=0.005921, val=0.007883; logMAE: train=-5.129319, val=-4.843050\n",
      "2023-01-12 04:40:54 (INFO): 1790000/3000000 (epoch 521): Loss: train=0.005852, val=0.007805; logMAE: train=-5.141014, val=-4.853052\n",
      "2023-01-12 04:46:42 (INFO): 1800000/3000000 (epoch 524): Loss: train=0.005765, val=0.007801; logMAE: train=-5.155995, val=-4.853512\n",
      "2023-01-12 04:52:31 (INFO): 1810000/3000000 (epoch 527): Loss: train=0.005809, val=0.007869; logMAE: train=-5.148411, val=-4.844862\n",
      "2023-01-12 04:58:19 (INFO): 1820000/3000000 (epoch 530): Loss: train=0.005694, val=0.007816; logMAE: train=-5.168389, val=-4.851552\n",
      "2023-01-12 05:04:08 (INFO): 1830000/3000000 (epoch 533): Loss: train=0.005730, val=0.007798; logMAE: train=-5.162081, val=-4.853847\n",
      "2023-01-12 05:09:56 (INFO): 1840000/3000000 (epoch 536): Loss: train=0.005688, val=0.007803; logMAE: train=-5.169370, val=-4.853220\n",
      "2023-01-12 05:15:44 (INFO): 1850000/3000000 (epoch 539): Loss: train=0.005588, val=0.007731; logMAE: train=-5.187183, val=-4.862461\n",
      "2023-01-12 05:21:33 (INFO): 1860000/3000000 (epoch 542): Loss: train=0.005683, val=0.007758; logMAE: train=-5.170300, val=-4.858996\n",
      "2023-01-12 05:27:21 (INFO): 1870000/3000000 (epoch 544): Loss: train=0.005491, val=0.007726; logMAE: train=-5.204607, val=-4.863179\n",
      "2023-01-12 05:33:09 (INFO): 1880000/3000000 (epoch 547): Loss: train=0.005503, val=0.007739; logMAE: train=-5.202531, val=-4.861436\n",
      "2023-01-12 05:38:57 (INFO): 1890000/3000000 (epoch 550): Loss: train=0.005404, val=0.007715; logMAE: train=-5.220542, val=-4.864544\n",
      "2023-01-12 05:44:45 (INFO): 1900000/3000000 (epoch 553): Loss: train=0.005419, val=0.007714; logMAE: train=-5.217823, val=-4.864725\n",
      "2023-01-12 05:50:34 (INFO): 1910000/3000000 (epoch 556): Loss: train=0.005338, val=0.007708; logMAE: train=-5.232953, val=-4.865445\n",
      "2023-01-12 05:56:22 (INFO): 1920000/3000000 (epoch 559): Loss: train=0.005364, val=0.007690; logMAE: train=-5.227955, val=-4.867884\n",
      "2023-01-12 06:02:10 (INFO): 1930000/3000000 (epoch 562): Loss: train=0.005376, val=0.007692; logMAE: train=-5.225814, val=-4.867555\n",
      "2023-01-12 06:07:59 (INFO): 1940000/3000000 (epoch 565): Loss: train=0.005271, val=0.007681; logMAE: train=-5.245480, val=-4.869013\n",
      "2023-01-12 06:13:47 (INFO): 1950000/3000000 (epoch 568): Loss: train=0.005276, val=0.007654; logMAE: train=-5.244533, val=-4.872534\n",
      "2023-01-12 06:19:35 (INFO): 1960000/3000000 (epoch 571): Loss: train=0.005234, val=0.007645; logMAE: train=-5.252587, val=-4.873739\n",
      "2023-01-12 06:25:23 (INFO): 1970000/3000000 (epoch 574): Loss: train=0.005244, val=0.007655; logMAE: train=-5.250740, val=-4.872363\n",
      "2023-01-12 06:31:11 (INFO): 1980000/3000000 (epoch 576): Loss: train=0.005189, val=0.007629; logMAE: train=-5.261297, val=-4.875782\n",
      "2023-01-12 06:36:59 (INFO): 1990000/3000000 (epoch 579): Loss: train=0.005082, val=0.007606; logMAE: train=-5.282012, val=-4.878826\n",
      "2023-01-12 06:42:47 (INFO): 2000000/3000000 (epoch 582): Loss: train=0.005060, val=0.007630; logMAE: train=-5.286293, val=-4.875718\n",
      "2023-01-12 06:48:36 (INFO): 2010000/3000000 (epoch 585): Loss: train=0.005033, val=0.007632; logMAE: train=-5.291790, val=-4.875432\n",
      "2023-01-12 06:54:24 (INFO): 2020000/3000000 (epoch 588): Loss: train=0.005047, val=0.007613; logMAE: train=-5.288937, val=-4.877920\n",
      "2023-01-12 07:00:13 (INFO): 2030000/3000000 (epoch 591): Loss: train=0.004944, val=0.007603; logMAE: train=-5.309502, val=-4.879238\n",
      "2023-01-12 07:06:01 (INFO): 2040000/3000000 (epoch 594): Loss: train=0.005047, val=0.007585; logMAE: train=-5.289000, val=-4.881549\n",
      "2023-01-12 07:11:49 (INFO): 2050000/3000000 (epoch 597): Loss: train=0.004998, val=0.007578; logMAE: train=-5.298662, val=-4.882514\n",
      "2023-01-12 07:17:38 (INFO): 2060000/3000000 (epoch 600): Loss: train=0.004966, val=0.007583; logMAE: train=-5.305069, val=-4.881792\n",
      "2023-01-12 07:23:26 (INFO): 2070000/3000000 (epoch 603): Loss: train=0.005018, val=0.007574; logMAE: train=-5.294705, val=-4.883059\n",
      "2023-01-12 07:29:14 (INFO): 2080000/3000000 (epoch 606): Loss: train=0.004851, val=0.007551; logMAE: train=-5.328517, val=-4.886088\n",
      "2023-01-12 07:35:02 (INFO): 2090000/3000000 (epoch 608): Loss: train=0.004792, val=0.007570; logMAE: train=-5.340866, val=-4.883564\n",
      "2023-01-12 07:40:51 (INFO): 2100000/3000000 (epoch 611): Loss: train=0.004748, val=0.007541; logMAE: train=-5.349980, val=-4.887391\n",
      "2023-01-12 07:46:39 (INFO): 2110000/3000000 (epoch 614): Loss: train=0.004735, val=0.007561; logMAE: train=-5.352780, val=-4.884704\n",
      "2023-01-12 07:52:27 (INFO): 2120000/3000000 (epoch 617): Loss: train=0.004716, val=0.007570; logMAE: train=-5.356708, val=-4.883547\n",
      "2023-01-12 07:58:15 (INFO): 2130000/3000000 (epoch 620): Loss: train=0.004672, val=0.007587; logMAE: train=-5.366102, val=-4.881318\n",
      "2023-01-12 08:04:04 (INFO): 2140000/3000000 (epoch 623): Loss: train=0.004726, val=0.007569; logMAE: train=-5.354579, val=-4.883715\n",
      "2023-01-12 08:09:52 (INFO): 2150000/3000000 (epoch 626): Loss: train=0.004657, val=0.007607; logMAE: train=-5.369418, val=-4.878749\n",
      "2023-01-12 08:15:40 (INFO): 2160000/3000000 (epoch 629): Loss: train=0.004675, val=0.007576; logMAE: train=-5.365585, val=-4.882739\n",
      "2023-01-12 08:21:29 (INFO): 2170000/3000000 (epoch 632): Loss: train=0.004608, val=0.007544; logMAE: train=-5.379971, val=-4.887010\n",
      "2023-01-12 08:27:17 (INFO): 2180000/3000000 (epoch 635): Loss: train=0.004596, val=0.007539; logMAE: train=-5.382580, val=-4.887605\n",
      "2023-01-12 08:33:06 (INFO): 2190000/3000000 (epoch 637): Loss: train=0.004548, val=0.007524; logMAE: train=-5.393088, val=-4.889642\n",
      "2023-01-12 08:38:54 (INFO): 2200000/3000000 (epoch 640): Loss: train=0.004563, val=0.007516; logMAE: train=-5.389784, val=-4.890689\n",
      "2023-01-12 08:44:42 (INFO): 2210000/3000000 (epoch 643): Loss: train=0.004526, val=0.007531; logMAE: train=-5.397842, val=-4.888692\n",
      "2023-01-12 08:50:31 (INFO): 2220000/3000000 (epoch 646): Loss: train=0.004492, val=0.007528; logMAE: train=-5.405393, val=-4.889116\n",
      "2023-01-12 08:56:19 (INFO): 2230000/3000000 (epoch 649): Loss: train=0.004441, val=0.007540; logMAE: train=-5.416893, val=-4.887486\n",
      "2023-01-12 09:02:07 (INFO): 2240000/3000000 (epoch 652): Loss: train=0.004421, val=0.007490; logMAE: train=-5.421362, val=-4.894208\n",
      "2023-01-12 09:07:55 (INFO): 2250000/3000000 (epoch 655): Loss: train=0.004462, val=0.007491; logMAE: train=-5.412091, val=-4.894052\n",
      "2023-01-12 09:13:43 (INFO): 2260000/3000000 (epoch 658): Loss: train=0.004411, val=0.007502; logMAE: train=-5.423602, val=-4.892570\n",
      "2023-01-12 09:19:31 (INFO): 2270000/3000000 (epoch 661): Loss: train=0.004424, val=0.007464; logMAE: train=-5.420782, val=-4.897658\n",
      "2023-01-12 09:25:19 (INFO): 2280000/3000000 (epoch 664): Loss: train=0.004371, val=0.007472; logMAE: train=-5.432783, val=-4.896614\n",
      "2023-01-12 09:31:07 (INFO): 2290000/3000000 (epoch 667): Loss: train=0.004335, val=0.007447; logMAE: train=-5.440937, val=-4.899941\n",
      "2023-01-12 09:36:55 (INFO): 2300000/3000000 (epoch 669): Loss: train=0.004322, val=0.007473; logMAE: train=-5.444082, val=-4.896416\n",
      "2023-01-12 09:42:44 (INFO): 2310000/3000000 (epoch 672): Loss: train=0.004297, val=0.007448; logMAE: train=-5.449766, val=-4.899814\n",
      "2023-01-12 09:48:32 (INFO): 2320000/3000000 (epoch 675): Loss: train=0.004277, val=0.007432; logMAE: train=-5.454519, val=-4.901951\n",
      "2023-01-12 09:54:20 (INFO): 2330000/3000000 (epoch 678): Loss: train=0.004249, val=0.007451; logMAE: train=-5.461080, val=-4.899394\n",
      "2023-01-12 10:00:08 (INFO): 2340000/3000000 (epoch 681): Loss: train=0.004281, val=0.007454; logMAE: train=-5.453597, val=-4.899033\n",
      "2023-01-12 10:05:57 (INFO): 2350000/3000000 (epoch 684): Loss: train=0.004220, val=0.007426; logMAE: train=-5.467806, val=-4.902725\n",
      "2023-01-12 10:11:46 (INFO): 2360000/3000000 (epoch 687): Loss: train=0.004225, val=0.007425; logMAE: train=-5.466843, val=-4.902871\n",
      "2023-01-12 10:17:34 (INFO): 2370000/3000000 (epoch 690): Loss: train=0.004200, val=0.007428; logMAE: train=-5.472618, val=-4.902458\n",
      "2023-01-12 10:23:23 (INFO): 2380000/3000000 (epoch 693): Loss: train=0.004230, val=0.007433; logMAE: train=-5.465480, val=-4.901778\n",
      "2023-01-12 10:29:11 (INFO): 2390000/3000000 (epoch 696): Loss: train=0.004158, val=0.007414; logMAE: train=-5.482657, val=-4.904395\n",
      "2023-01-12 10:35:00 (INFO): 2400000/3000000 (epoch 699): Loss: train=0.004152, val=0.007446; logMAE: train=-5.484117, val=-4.900085\n",
      "2023-01-12 10:40:48 (INFO): 2410000/3000000 (epoch 701): Loss: train=0.004158, val=0.007445; logMAE: train=-5.482729, val=-4.900222\n",
      "2023-01-12 10:46:36 (INFO): 2420000/3000000 (epoch 704): Loss: train=0.004092, val=0.007417; logMAE: train=-5.498774, val=-4.903919\n",
      "2023-01-12 10:52:25 (INFO): 2430000/3000000 (epoch 707): Loss: train=0.004063, val=0.007429; logMAE: train=-5.505948, val=-4.902390\n",
      "2023-01-12 10:58:13 (INFO): 2440000/3000000 (epoch 710): Loss: train=0.004081, val=0.007394; logMAE: train=-5.501478, val=-4.907027\n",
      "2023-01-12 11:04:01 (INFO): 2450000/3000000 (epoch 713): Loss: train=0.004011, val=0.007387; logMAE: train=-5.518596, val=-4.908029\n",
      "2023-01-12 11:09:50 (INFO): 2460000/3000000 (epoch 716): Loss: train=0.004021, val=0.007368; logMAE: train=-5.516325, val=-4.910592\n",
      "2023-01-12 11:15:38 (INFO): 2470000/3000000 (epoch 719): Loss: train=0.003994, val=0.007369; logMAE: train=-5.522934, val=-4.910460\n",
      "2023-01-12 11:21:26 (INFO): 2480000/3000000 (epoch 722): Loss: train=0.004005, val=0.007370; logMAE: train=-5.520182, val=-4.910307\n",
      "2023-01-12 11:27:14 (INFO): 2490000/3000000 (epoch 725): Loss: train=0.003985, val=0.007374; logMAE: train=-5.525252, val=-4.909853\n",
      "2023-01-12 11:33:03 (INFO): 2500000/3000000 (epoch 728): Loss: train=0.003945, val=0.007349; logMAE: train=-5.535282, val=-4.913194\n",
      "2023-01-12 11:38:51 (INFO): 2510000/3000000 (epoch 731): Loss: train=0.003918, val=0.007348; logMAE: train=-5.542080, val=-4.913356\n",
      "2023-01-12 11:44:40 (INFO): 2520000/3000000 (epoch 733): Loss: train=0.003950, val=0.007344; logMAE: train=-5.533967, val=-4.913874\n",
      "2023-01-12 11:50:29 (INFO): 2530000/3000000 (epoch 736): Loss: train=0.003920, val=0.007338; logMAE: train=-5.541578, val=-4.914672\n",
      "2023-01-12 11:56:17 (INFO): 2540000/3000000 (epoch 739): Loss: train=0.003901, val=0.007346; logMAE: train=-5.546603, val=-4.913558\n",
      "2023-01-12 12:02:06 (INFO): 2550000/3000000 (epoch 742): Loss: train=0.003880, val=0.007337; logMAE: train=-5.551846, val=-4.914845\n",
      "2023-01-12 12:07:54 (INFO): 2560000/3000000 (epoch 745): Loss: train=0.003879, val=0.007318; logMAE: train=-5.552126, val=-4.917387\n",
      "2023-01-12 12:13:43 (INFO): 2570000/3000000 (epoch 748): Loss: train=0.003829, val=0.007321; logMAE: train=-5.565203, val=-4.916957\n",
      "2023-01-12 12:19:31 (INFO): 2580000/3000000 (epoch 751): Loss: train=0.003882, val=0.007321; logMAE: train=-5.551459, val=-4.917036\n",
      "2023-01-12 12:25:20 (INFO): 2590000/3000000 (epoch 754): Loss: train=0.003872, val=0.007321; logMAE: train=-5.553856, val=-4.916992\n",
      "2023-01-12 12:31:08 (INFO): 2600000/3000000 (epoch 757): Loss: train=0.003845, val=0.007312; logMAE: train=-5.560976, val=-4.918236\n",
      "2023-01-12 12:36:57 (INFO): 2610000/3000000 (epoch 760): Loss: train=0.003818, val=0.007315; logMAE: train=-5.567990, val=-4.917857\n",
      "2023-01-12 12:42:45 (INFO): 2620000/3000000 (epoch 763): Loss: train=0.003780, val=0.007312; logMAE: train=-5.577935, val=-4.918190\n",
      "2023-01-12 12:48:34 (INFO): 2630000/3000000 (epoch 765): Loss: train=0.003788, val=0.007308; logMAE: train=-5.575854, val=-4.918819\n",
      "2023-01-12 12:54:22 (INFO): 2640000/3000000 (epoch 768): Loss: train=0.003764, val=0.007301; logMAE: train=-5.582181, val=-4.919761\n",
      "2023-01-12 13:00:10 (INFO): 2650000/3000000 (epoch 771): Loss: train=0.003743, val=0.007311; logMAE: train=-5.587915, val=-4.918416\n",
      "2023-01-12 13:05:58 (INFO): 2660000/3000000 (epoch 774): Loss: train=0.003741, val=0.007294; logMAE: train=-5.588311, val=-4.920650\n",
      "2023-01-12 13:11:47 (INFO): 2670000/3000000 (epoch 777): Loss: train=0.003773, val=0.007296; logMAE: train=-5.579988, val=-4.920456\n",
      "2023-01-12 13:17:35 (INFO): 2680000/3000000 (epoch 780): Loss: train=0.003745, val=0.007293; logMAE: train=-5.587406, val=-4.920857\n",
      "2023-01-12 13:23:24 (INFO): 2690000/3000000 (epoch 783): Loss: train=0.003703, val=0.007283; logMAE: train=-5.598634, val=-4.922218\n",
      "2023-01-12 13:29:12 (INFO): 2700000/3000000 (epoch 786): Loss: train=0.003710, val=0.007281; logMAE: train=-5.596720, val=-4.922466\n",
      "2023-01-12 13:35:00 (INFO): 2710000/3000000 (epoch 789): Loss: train=0.003686, val=0.007289; logMAE: train=-5.603334, val=-4.921344\n",
      "2023-01-12 13:40:49 (INFO): 2720000/3000000 (epoch 792): Loss: train=0.003647, val=0.007288; logMAE: train=-5.613848, val=-4.921552\n",
      "2023-01-12 13:46:37 (INFO): 2730000/3000000 (epoch 795): Loss: train=0.003668, val=0.007276; logMAE: train=-5.608025, val=-4.923214\n",
      "2023-01-12 13:52:25 (INFO): 2740000/3000000 (epoch 797): Loss: train=0.003631, val=0.007263; logMAE: train=-5.618376, val=-4.924897\n",
      "2023-01-12 13:58:13 (INFO): 2750000/3000000 (epoch 800): Loss: train=0.003657, val=0.007265; logMAE: train=-5.611146, val=-4.924689\n",
      "2023-01-12 14:04:02 (INFO): 2760000/3000000 (epoch 803): Loss: train=0.003599, val=0.007265; logMAE: train=-5.627217, val=-4.924688\n",
      "2023-01-12 14:09:50 (INFO): 2770000/3000000 (epoch 806): Loss: train=0.003618, val=0.007271; logMAE: train=-5.621726, val=-4.923802\n",
      "2023-01-12 14:15:39 (INFO): 2780000/3000000 (epoch 809): Loss: train=0.003608, val=0.007263; logMAE: train=-5.624647, val=-4.924969\n",
      "2023-01-12 14:21:27 (INFO): 2790000/3000000 (epoch 812): Loss: train=0.003593, val=0.007265; logMAE: train=-5.628798, val=-4.924648\n",
      "2023-01-12 14:27:15 (INFO): 2800000/3000000 (epoch 815): Loss: train=0.003593, val=0.007259; logMAE: train=-5.628771, val=-4.925470\n",
      "2023-01-12 14:33:04 (INFO): 2810000/3000000 (epoch 818): Loss: train=0.003560, val=0.007257; logMAE: train=-5.637871, val=-4.925852\n",
      "2023-01-12 14:38:53 (INFO): 2820000/3000000 (epoch 821): Loss: train=0.003568, val=0.007270; logMAE: train=-5.635787, val=-4.924026\n",
      "2023-01-12 14:44:41 (INFO): 2830000/3000000 (epoch 824): Loss: train=0.003537, val=0.007293; logMAE: train=-5.644570, val=-4.920905\n",
      "2023-01-12 14:50:29 (INFO): 2840000/3000000 (epoch 827): Loss: train=0.003544, val=0.007264; logMAE: train=-5.642443, val=-4.924863\n",
      "2023-01-12 14:56:18 (INFO): 2850000/3000000 (epoch 829): Loss: train=0.003517, val=0.007248; logMAE: train=-5.650030, val=-4.926963\n",
      "2023-01-12 15:02:07 (INFO): 2860000/3000000 (epoch 832): Loss: train=0.003526, val=0.007240; logMAE: train=-5.647529, val=-4.928149\n",
      "2023-01-12 15:07:55 (INFO): 2870000/3000000 (epoch 835): Loss: train=0.003523, val=0.007265; logMAE: train=-5.648556, val=-4.924743\n",
      "2023-01-12 15:13:44 (INFO): 2880000/3000000 (epoch 838): Loss: train=0.003509, val=0.007286; logMAE: train=-5.652407, val=-4.921811\n",
      "2023-01-12 15:19:32 (INFO): 2890000/3000000 (epoch 841): Loss: train=0.003509, val=0.007270; logMAE: train=-5.652317, val=-4.923948\n",
      "2023-01-12 15:25:20 (INFO): 2900000/3000000 (epoch 844): Loss: train=0.003463, val=0.007266; logMAE: train=-5.665658, val=-4.924535\n",
      "2023-01-12 15:31:09 (INFO): 2910000/3000000 (epoch 847): Loss: train=0.003453, val=0.007249; logMAE: train=-5.668388, val=-4.926876\n",
      "2023-01-12 15:36:58 (INFO): 2920000/3000000 (epoch 850): Loss: train=0.003461, val=0.007249; logMAE: train=-5.666095, val=-4.926844\n",
      "2023-01-12 15:42:46 (INFO): 2930000/3000000 (epoch 853): Loss: train=0.003435, val=0.007213; logMAE: train=-5.673722, val=-4.931851\n",
      "2023-01-12 15:48:35 (INFO): 2940000/3000000 (epoch 856): Loss: train=0.003422, val=0.007277; logMAE: train=-5.677463, val=-4.923031\n",
      "2023-01-12 15:54:23 (INFO): 2950000/3000000 (epoch 859): Loss: train=0.003441, val=0.007291; logMAE: train=-5.672035, val=-4.921103\n",
      "2023-01-12 16:00:11 (INFO): 2960000/3000000 (epoch 861): Loss: train=0.003427, val=0.007289; logMAE: train=-5.676007, val=-4.921366\n",
      "2023-01-12 16:06:00 (INFO): 2970000/3000000 (epoch 864): Loss: train=0.003451, val=0.007264; logMAE: train=-5.669135, val=-4.924825\n",
      "2023-01-12 16:11:49 (INFO): 2980000/3000000 (epoch 867): Loss: train=0.003402, val=0.007260; logMAE: train=-5.683264, val=-4.925423\n",
      "2023-01-12 16:17:37 (INFO): 2990000/3000000 (epoch 870): Loss: train=0.003407, val=0.007272; logMAE: train=-5.681870, val=-4.923736\n",
      "2023-01-12 16:23:25 (INFO): 3000000/3000000 (epoch 873): Loss: train=0.003381, val=0.007277; logMAE: train=-5.689722, val=-4.923076\n"
     ]
    }
   ],
   "source": [
    "with summary_writer.as_default():\n",
    "    steps_per_epoch = int(np.ceil(num_train / batch_size))\n",
    "\n",
    "    if ckpt_restored is not None:\n",
    "        step_init = ckpt.step.numpy()\n",
    "    else:\n",
    "        step_init = 1\n",
    "    for step in range(step_init, num_steps + 1):\n",
    "        # Update step number\n",
    "        ckpt.step.assign(step)\n",
    "        tf.summary.experimental.set_step(step)\n",
    "\n",
    "        # Perform training step\n",
    "        trainer.train_on_batch(train['dataset_iter'], train['metrics'])\n",
    "\n",
    "        # Save progress\n",
    "        if (step % save_interval == 0):\n",
    "            manager.save()\n",
    "\n",
    "        # Evaluate model and log results\n",
    "        if (step % evaluation_interval == 0):\n",
    "\n",
    "            # Save backup variables and load averaged variables\n",
    "            trainer.save_variable_backups()\n",
    "            trainer.load_averaged_variables()\n",
    "\n",
    "            # Compute results on the validation set\n",
    "            for i in range(int(np.ceil(num_valid / batch_size))):\n",
    "                trainer.test_on_batch(validation['dataset_iter'], validation['metrics'])\n",
    "\n",
    "            # Update and save best result\n",
    "            if validation['metrics'].mean_mae < metrics_best['mean_mae_val']:\n",
    "                metrics_best['step'] = step\n",
    "                metrics_best.update(validation['metrics'].result())\n",
    "\n",
    "                np.savez(best_loss_file, **metrics_best)\n",
    "                model.save_weights(best_ckpt_file)\n",
    "\n",
    "            for key, val in metrics_best.items():\n",
    "                if key != 'step':\n",
    "                    tf.summary.scalar(key + '_best', val)\n",
    "\n",
    "            epoch = step // steps_per_epoch\n",
    "            logging.info(\n",
    "                f\"{step}/{num_steps} (epoch {epoch+1}): \"\n",
    "                f\"Loss: train={train['metrics'].loss:.6f}, val={validation['metrics'].loss:.6f}; \"\n",
    "                f\"logMAE: train={train['metrics'].mean_log_mae:.6f}, \"\n",
    "                f\"val={validation['metrics'].mean_log_mae:.6f}\")\n",
    "\n",
    "            train['metrics'].write()\n",
    "            validation['metrics'].write()\n",
    "\n",
    "            train['metrics'].reset_states()\n",
    "            validation['metrics'].reset_states()\n",
    "\n",
    "            # Restore backup variables\n",
    "            trainer.restore_variable_backups()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dimenet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "5a7dbe99590479bddff41be29e10fb3c15bf5c168951678a982666ae17423fa3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
